{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENIA Filtered Type Analysis\n",
    "The GENIA dataset used in many NER papers, including the DyGIE++ paper, is reduced to only 5 of the original 47 entity mention types: DNA, RNA, Protein, Cell Line, and Cell Type. This explains much of the unexpected poor performance of the GENIA model; the model performs exceptionally well on the DNA, RNA and Protein types, and our PICKLE dataset doesn't have many Cell annotations at all. Here, we look at the performacne of the GENIA model if we filter the PICKLE test set down to just those types represented in the GENIA model before evaluation. We perform this filtering using the script `filter_pickle_to_GENIA.py` in the `annotation/abstract_scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "### Performance data\n",
    "First, let's read in the original performance data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_perf_seedev = pd.read_csv('../data/straying_off_topic_data/model_output/dygiepp/15Jul2023_all_on_pickle_but_seedev/performance/17Jul2023_seedev_on_pickle_performance_no_labels.csv')\n",
    "orig_perf_all = pd.read_csv('../data/straying_off_topic_data/model_output/dygiepp/15Jul2023_all_on_pickle_but_seedev/performance/15Jul2023_all_on_pickle_no_seedev_performance.csv')\n",
    "orig_perf_all = pd.concat([orig_perf_seedev, orig_perf_all])\n",
    "orig_perf_all['model'] = orig_perf_all['pred_file'].str.split('_').str[-2]\n",
    "orig_perf_all = orig_perf_all.drop(index=1).reset_index(drop=True)\n",
    "orig_perf_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_perf = pd.read_csv('../data/straying_off_topic_data/model_output/dygiepp/15Jul2023_all_on_pickle_but_seedev/performance/15Jul2023_all_on_pickle_no_seedev_performance.csv')\n",
    "orig_perf['model'] = orig_perf['pred_file'].str.split('_').str[-2]\n",
    "orig_perf = orig_perf[orig_perf['model'].isin(['genia', 'genia-lightweight'])]\n",
    "orig_perf = orig_perf.reset_index(drop=True)\n",
    "orig_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's read in the performance data for the filtered PICKLE test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../data/straying_off_topic_data/model_output/mismatch_analysis/'\n",
    "filepaths = {\n",
    "    'filtered_with_types': 'PICKLE_down_to_GENIA_eval_with_bootstrap_with_types.csv',\n",
    "    'filtered_without_types': 'PICKLE_down_to_GENIA_eval_with_bootstrap_without_types.csv'\n",
    "}\n",
    "evals = {k: pd.read_csv(f'{prefix}{v}') for k,v in filepaths.items()}\n",
    "for k in evals.keys():\n",
    "    evals[k]['model'] = evals[k]['pred_file'].str.split('_').str[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evals['filtered_without_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add orig to new\n",
    "evals['orig_perfs'] = orig_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance plots\n",
    "Let's take a look at the performance differences across conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CIs(df, kind='F1'):\n",
    "    \"\"\"\n",
    "    literal_evals the CI strings in a given df.\n",
    "    \"\"\"\n",
    "    ent_CIs = df[f\"ent_{kind}_CI\"].apply(lambda x: literal_eval(str(x)))\n",
    "    ent_CIs = pd.DataFrame([[df[f'ent_{kind}'][i] - val[0] for i, val in enumerate(ent_CIs)], [val[1] - df[f'ent_{kind}'][i] for i, val in enumerate(ent_CIs)]])\n",
    "    col_names = {i: df.loc[i, 'model'] for i in range(len(ent_CIs))}\n",
    "    ent_CIs = ent_CIs.rename(columns=col_names) \n",
    "\n",
    "    return ent_CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_plot_data = {'x': [], 'genia_f1': [], 'genialight_f1': [], 'genia_ci': [], 'genialight_ci': []}\n",
    "for name, perf_df in evals.items():\n",
    "    CIs = process_CIs(perf_df)\n",
    "    F1s = perf_df[['model', 'ent_F1']]\n",
    "    perf_plot_data['x'].append(name)\n",
    "    perf_plot_data['genia_f1'].append(F1s.loc[F1s['model'] == 'genia', 'ent_F1'].values[0])\n",
    "    perf_plot_data['genialight_f1'].append(F1s.loc[F1s['model'] == 'genia-lightweight', 'ent_F1'].values[0])\n",
    "    perf_plot_data['genia_ci'].append(CIs['genia'])\n",
    "    perf_plot_data['genialight_ci'].append(CIs['genia-lightweight'])\n",
    "perf_plot_data['genia_ci'] = pd.concat(perf_plot_data['genia_ci'], axis=1)\n",
    "perf_plot_data['genialight_ci'] = pd.concat(perf_plot_data['genialight_ci'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a name mapping for semantic labels\n",
    "name_map = {\n",
    "    'filtered_with_types': 'Filtered PICKLE\\nWith types',\n",
    "    'filtered_without_types': 'Filtered PICKLE\\nWithout types',\n",
    "    'orig_perfs': 'All PICKLE\\nWithout types'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_plot_data['semantic_x'] = pd.Series([name_map[x] for x in perf_plot_data['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical x values to allow offsets\n",
    "x_dict = {mod:i for i,mod in enumerate(perf_plot_data[\"x\"])}\n",
    "x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_plot_data['num_x'] = pd.Series([x_dict[x] for x in perf_plot_data['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=perf_plot_data['num_x'] - 0.1, y=perf_plot_data['genia_f1'], yerr=perf_plot_data['genia_ci'], fmt='o', color='purple', label='GENIA')\n",
    "plt.errorbar(x=perf_plot_data['num_x'] + 0.1, y=perf_plot_data['genialight_f1'], yerr=perf_plot_data['genialight_ci'], fmt='^', color='orange', label='GENIA Lightweight')\n",
    "\n",
    "plt.xticks(perf_plot_data['num_x'], perf_plot_data['semantic_x'], size=12, ha='center')\n",
    "plt.xlabel('Evaluation Dataset', size=14, labelpad=10)\n",
    "plt.ylabel('Entity F1', size=14, labelpad=10)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the performances of all of the models next to both original and filtered GENIA without types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_perf_all = orig_perf_all.sort_values('ent_F1')\n",
    "orig_perf_all = orig_perf_all.reset_index(drop=True)\n",
    "filtered_perf = evals['filtered_without_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ent_only_CIs(df, kind='F1'):\n",
    "    \"\"\"\n",
    "    literal_evals the CI strings in a given df, and returns two sets of CIs,\n",
    "    one for entities and one for relations.\n",
    "    \"\"\"\n",
    "    ent_CIs = df[f\"ent_{kind}_CI\"].apply(lambda x: literal_eval(str(x)))\n",
    "    ent_CIs = pd.DataFrame([[df[f'ent_{kind}'][i] - val[0] for i, val in enumerate(ent_CIs)], [val[1] - df[f'ent_{kind}'][i] for i, val in enumerate(ent_CIs)]])\n",
    "\n",
    "    return ent_CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_no_filter_ent_CIs = process_ent_only_CIs(orig_perf_all)\n",
    "filter_ent_CIs = process_ent_only_CIs(filtered_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {mod:i for i,mod in enumerate(orig_perf_all[\"model\"].values.tolist())}\n",
    "orig_perf_all[\"x\"] = orig_perf_all[\"model\"].map(x_dict)\n",
    "filtered_perf[\"x\"] = filtered_perf[\"model\"].map(x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_label_key = {'chemprot': 'ChemProt',\n",
    "         'scierc': 'SciERC',\n",
    "         'bioinfer': 'BioInfer',\n",
    "         'genia': 'GENIA',\n",
    "         'pickle': 'PICKLE',\n",
    "         'scierc-lightweight': 'SciERC lightweight',\n",
    "         'genia-lightweight': 'GENIA lightweight',\n",
    "         'ace05-relation': 'ACE05',\n",
    "         'seedev': 'SeeDev'}\n",
    "filtered_label_key = {'genia': 'GENIA on filtered PICKLE',\n",
    "                     'genia-lightweight': 'GENIA lightweight on filtered PICKLE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_x = [orig_label_key[mod] for mod in orig_perf_all[\"model\"]]\n",
    "\n",
    "plt.errorbar(x=orig_perf_all[\"x\"] + 0.1, y=orig_perf_all[\"ent_F1\"], yerr=all_no_filter_ent_CIs, fmt=\"o\", color='#E66100', label='On original PICKLE test set')\n",
    "plt.errorbar(x=filtered_perf[\"x\"] - 0.1, y=filtered_perf[\"ent_F1\"], yerr=filter_ent_CIs, fmt=\"^\", color='#5D3A9B', label='On GENIA-filtered PICKLE test set')\n",
    "plt.xticks(orig_perf_all[\"x\"], name_x, size=12, rotation=60, ha='right')\n",
    "plt.xlabel('Model', size=14)\n",
    "plt.ylabel('F1', y=0.6,  size=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring poor performance with types\n",
    "The poor performance of the GENIA model on the filtered PICKLE dataset when evaluated with types is unexpected. Let's dig into that here to make sure nothing untoward is happening.\n",
    "\n",
    "Given that the performance when no types are considered is quite high, it's likely that the issue is GENIA consistently misidentnifying types on entities that are otherwise correctly predicted. Let's quantify that supposition by checking what percentage of false negatives are negative becuase of type and not because of span boundaries.\n",
    "\n",
    "To do this, we'll actually look at the mismatches from where the types were ignored, and look at the false negatives versus the true positives where the type of the prediction and the gold don't match. This allows us to separate the false negatives by whether or not it was due to type or boundary in a straighforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_types_mismatches = pd.read_csv('../data/straying_off_topic_data/model_output/mismatch_analysis/PICKLE_down_to_GENIA_eval_without_bootstrap_without_types_MISMATCHES.csv')\n",
    "without_types_mismatches['model_shorthand'] = without_types_mismatches['model'].str.split('_').str[-2]\n",
    "without_types_mismatches['gold_ent_type'] = without_types_mismatches['gold_ent_type'].str.lower()\n",
    "without_types_mismatches['pred_ent_type'] = without_types_mismatches['pred_ent_type'].str.lower()\n",
    "without_types_mismatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stats_with_type = {}\n",
    "for model in ['genia', 'genia-lightweight']:\n",
    "    stats = {}\n",
    "    false_neg_type = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 1) & (without_types_mismatches['gold_ent_type'] != without_types_mismatches['pred_ent_type'])]\n",
    "    stats['num_false_neg_type'] = false_neg_type.shape[0]\n",
    "    false_neg_boundary = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 0)]\n",
    "    stats['num_false_neg_boundary'] = false_neg_boundary.shape[0]\n",
    "    false_pos = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 2)]\n",
    "    stats['num_false_pos'] = false_pos.shape[0]\n",
    "    true_pos_with_type = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 1) & (without_types_mismatches['gold_ent_type'] == without_types_mismatches['pred_ent_type'])]\n",
    "    stats['num_true_pos_with_type'] = true_pos_with_type.shape[0]\n",
    "    all_preds = without_types_mismatches[without_types_mismatches['model_shorthand'] == model]\n",
    "    stats['total_preds'] = all_preds.shape[0]\n",
    "    neg_stats_with_type[model] = stats\n",
    "    \n",
    "neg_stats_with_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_stats_without_type = {}\n",
    "for model in ['genia', 'genia-lightweight']:\n",
    "    stats = {}\n",
    "    false_neg_boundary = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 0)]\n",
    "    stats['num_false_neg_boundary'] = false_neg_boundary.shape[0]\n",
    "    false_pos = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 2)]\n",
    "    stats['num_false_pos'] = false_pos.shape[0]\n",
    "    true_pos = without_types_mismatches[(without_types_mismatches['model_shorthand'] == model) & (without_types_mismatches['mismatch_type'] == 1)]\n",
    "    stats['num_true_pos'] = true_pos.shape[0]\n",
    "    all_preds = without_types_mismatches[without_types_mismatches['model_shorthand'] == model]\n",
    "    stats['total_preds'] = all_preds.shape[0]\n",
    "    neg_stats_without_type[model] = stats\n",
    "    \n",
    "neg_stats_without_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common-sense check that the misidentification of types causes the discrepancy in performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without type\n",
    "tp = neg_stats_without_type['genia']['num_true_pos']\n",
    "fp = neg_stats_without_type['genia']['num_false_pos']\n",
    "fn = neg_stats_without_type['genia']['num_false_neg_boundary']\n",
    "2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider types, anything that had a correct boundary but an incorrect type, rather than being a true positive, is now both a false negative (because it's an element of the gold standard that doesn't match any prediction) and a false positive (because it's a prediction that doesn't match anything in the gold standard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With type\n",
    "tp = neg_stats_with_type['genia']['num_true_pos_with_type']\n",
    "fp = neg_stats_with_type['genia']['num_false_pos'] + neg_stats_with_type['genia']['num_false_neg_type']\n",
    "fn = neg_stats_with_type['genia']['num_false_neg_boundary'] + neg_stats_with_type['genia']['num_false_neg_type']\n",
    "2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is certainly suspicious! Let's read in the actual mismatches dataframe with types and re-calculate the F1 from there to make sure we didn't do anything wrong here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_types_mismatches = pd.read_csv('../data/straying_off_topic_data/model_output/mismatch_analysis/PICKLE_down_to_GENIA_eval_without_bootstrap_with_types_MISMATCHES.csv')\n",
    "with_types_mismatches['model_shorthand'] = with_types_mismatches['model'].str.split('_').str[-2]\n",
    "with_types_mismatches['gold_ent_type'] = with_types_mismatches['gold_ent_type'].str.lower()\n",
    "with_types_mismatches['pred_ent_type'] = with_types_mismatches['pred_ent_type'].str.lower()\n",
    "with_types_mismatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With type\n",
    "tp = with_types_mismatches[(with_types_mismatches['model_shorthand'] == 'genia') & (with_types_mismatches['mismatch_type'] == 1)].shape[0]\n",
    "fp = with_types_mismatches[(with_types_mismatches['model_shorthand'] == 'genia') & (with_types_mismatches['mismatch_type'] == 2)].shape[0]\n",
    "fn = with_types_mismatches[(with_types_mismatches['model_shorthand'] == 'genia') & (with_types_mismatches['mismatch_type'] == 0)].shape[0]\n",
    "2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is what we expected to see. So what else is different between the true positive and false negative categories when we look at types vs don't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mismatches = with_types_mismatches.merge(without_types_mismatches, how='outer', on=['doc_key', 'sent_num', 'gold_ent_list', 'gold_ent_type', 'pred_ent_list', 'pred_ent_type', 'model', 'model_shorthand'], suffixes=['_with_type', '_without_type'])\n",
    "print(all_mismatches[all_mismatches['model_shorthand'] == 'genia'].shape)\n",
    "all_mismatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mismatch_type_genia = all_mismatches[(all_mismatches['model_shorthand'] == 'genia') & (all_mismatches['mismatch_type_with_type'] != all_mismatches['mismatch_type_without_type'])]\n",
    "diff_mismatch_type_genia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pickle",
   "language": "python",
   "name": "pickle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
